１：
正则化是怎么运行的


我们呢要做的就是　对代价函数

修改线性回归的代价函数　来缩小所有参数

因为我不知道选择哪一个去缩小

修改后的式子如下：
　J(thea) = 1/2m [sum(1-m)(h(x^i)-y^i)^2　＋　lmuda sum(1-m)thea_j^2]

新添加的式子　是为了缩小thea

我们没有给thea_0增加惩罚项


lmuda被称为正则化参数

后面加的叫正则化项

lmuda的作用就是控制两个不同目标之间的取舍

控制两个目标之间的平衡关系

第一个目标是式子前面那个　　它是为了尽可能更好的拟合

第二个目标是式子后面那个　　它是为了使得thea尽可能的小　

而lmuda是为了控制两个目标

保证模型相对简单　
避免出现过拟合的情况



在正则化的线性回归中

如果正则化参数lmuda被设很大
那么就是说　对于thea3 thea4．．．．等等的惩罚太大
欠拟合

偏见性太强












