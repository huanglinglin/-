梯度下降算法中的实用技巧

特征缩放

多个特征 
确保这些特征都处于一个相近的范围
确保不同特征的取值在相近的范围内 这样梯度下降法就能更快收敛


两个特征的问题 x1 x2

x1  = 0-2000

x2 = 1-5 

参数 thea1 thea2 不考虑thea0

如果两个特征的取值差异过大的话 导致的是画出来的代价函数图
会呈现一种扁长形 非常歪斜椭圆

如果这样运行代价函数的话
会导致你的梯度下降算法会运行很长一段时间的
所以我们要进行特征缩放 来回波动

越细长越慢

使特征值的数目  在0-1之间

特征缩放  我们把取值约束到-1 到+1 之间

不一定要是 -1 - +1 之间 只要差不就行  比如 0-3之间

不能太大
不能太小

考虑 -3 - +3 
-1/3 - 1/3


均值归一化

减去均值  算出的特征值

x1 = x1 - u1 /s1

s1 = max-min
s1 = 标准差

转化为相近范围即可 为了更快 

=================================================================


学习率arfa 

认为的调试是什么
和一些小技巧来确保梯度下降是正常工作
如何让选择arfa


梯度下降算法的工作就是  找到一个thea值
并且希望能够最小化代价函数J(thea)

确认算法正常执行

x 是迭代次数

到后面会趋于平缓   正常工作 是每一次迭代都下降
用处在于：
判断有没有必要继续下降  和判断是否收敛 

收敛次数  每个问题不一样 
有的多 有的少 

画出曲线来判断是否已经收敛了


收敛测试  

自动收敛测试 一种算法

小于了一个很小的值  则收敛
但是找到一个合适的临界值很难

倾向于看这种曲线  
还可以警告你  是否算法正常工作

J(thea) 与次数的图像上升 常见原因是arfa过大
减小则行


J(thea) 与次数的图像反复下降上升 解决方法就是将arfa值变小



try许多arfa值  10倍  在绘制曲线

选择使得快速下降的arfa值

0.001 0.003 0.01 0.03 0.1 0.3 1 
在最大值和最小值中间取值
找到合适的学习率


